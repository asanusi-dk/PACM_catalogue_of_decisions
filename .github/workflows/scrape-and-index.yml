
name: Scrape Catalogue and Preserve Manual Fields

on:
  workflow_dispatch:
    inputs:
      run_scraper:
        description: 'Run scraper step (true) or just preserve/commit existing file (false)'
        required: false
        default: 'true'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Save previous catalogue (for manual fields)
        run: |
          mkdir -p /tmp
          if [ -f data/a64_catalogue.json ]; then
            cp data/a64_catalogue.json /tmp/old_catalogue.json
          else
            echo "[]" > /tmp/old_catalogue.json
          fi

      - name: Run scraper (if present)
        if: ${{ inputs.run_scraper == 'true' }}
        run: |
          if [ -f scripts/build_catalogue.py ]; then
            echo "Running scripts/build_catalogue.py"
            python3 scripts/build_catalogue.py
          else
            echo "No scripts/build_catalogue.py found; skipping scrape step."
          fi

      - name: Ensure catalogue file exists
        run: |
          mkdir -p data
          if [ ! -f data/a64_catalogue.json ]; then
            echo "[]"> data/a64_catalogue.json
          fi

      - name: Preserve section/subsection/notes from previous file
        run: |
          python - << 'PY'
          import json, pathlib
          OUT = pathlib.Path("data/a64_catalogue.json")
          OLD = pathlib.Path("/tmp/old_catalogue.json")
          fresh = json.loads(OUT.read_text(encoding="utf-8")) if OUT.exists() else []
          old   = json.loads(OLD.read_text(encoding="utf-8")) if OLD.exists() else []

          by_url    = {d.get("url"): d for d in old if d.get("url")}
          by_sig    = {(d.get("symbol"), d.get("title")): d for d in old if d.get("symbol") and d.get("title")}
          by_title  = {d.get("title"): d for d in old if d.get("title")}

          def find_old(n):
            if n.get("url") and n["url"] in by_url: return by_url[n["url"]]
            key = (n.get("symbol"), n.get("title"))
            if key in by_sig: return by_sig[key]
            if n.get("title") in by_title: return by_title[n["title"]]
            return None

          KEEP = ("section","subsection","notes")
          def keep_manual(old_rec, new_rec):
            if not old_rec: return new_rec
            for k in KEEP:
              v = old_rec.get(k)
              if v not in (None, "", []):
                new_rec[k] = v
            return new_rec

          merged = [keep_manual(find_old(n), n) for n in fresh]

          order = ["title","url","symbol","version","date","section","subsection","notes"]
          def order_fields(d):
            return {**{k:d[k] for k in order if k in d}, **{k:v for k,v in d.items() if k not in order}}

          merged = [order_fields(d) for d in merged]
          OUT.write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding="utf-8")
          print(f"Preserved manual fields for {len(merged)} items")
          PY

      - name: Commit updated catalogue
        uses: EndBug/add-and-commit@v9
        with:
          add: "data/a64_catalogue.json"
          message: "chore: scrape & preserve manual fields (section/subsection/notes)"
          default_author: github_actions
